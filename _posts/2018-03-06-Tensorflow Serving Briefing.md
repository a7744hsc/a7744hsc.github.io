---
layout: post
title:  "Tensorflow Serving Briefing"
date:   2018-03-06 22:00:00 +0800
categories: Machine Learning
---
先写中文吧。

Tensorflow Serving是google tensorflow开源项目下的一个子项目，官方的对Tensorflow Serving的介绍如下：

> TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. 

可以看到，TF serving设计宗旨就是高性能，高灵活性，为生产环境而生。所以当你已经有了训练好的模型，想要和应用系统进行集成的时候，可以考虑使用TF serving。对tensorflow模型，TF serving基本做到开箱即用，使用它可以帮你省去一些API开发成本。所以如果你正在使用tensorflow，那我强烈介意你了解一下TF serving。

<!-- 使用TF Serving具体有什么优势？我尝试列出我心中的几点理解：

  - 简化模型部署，升级模型时不改变服务器结构和API，只需将模型拷贝到指定文件夹下即可。
  - 不但可以直接与Tensorflow模型集成，也可以和其他模型一起使用（将模型编译成servable）。
  - 可以同时提供多个模型版本API -->

要学习使用TF serving，最好的材料当然还是官方[文档][official-doc],但是在实践过程中我或多或少遇到一些问题，这里把他们记录下来作为对官方文档做一个补充。

当你已经有了一个可用的模型一般只需要做下面两件事情即可将它部署到TF serving上，不过一般模型部署好之后还需要用一个客户端程序能够访问调用模型服务，所以本教程分三步：

  1. [将模型导出成SavedModel，一种TF serving使用的模型格式。](#step-1)
  2. [安装TF serving，用正确的参数将它启动。](#step-2)
  3. [开发一个简易客户端来调用模型服务](#step-3)

### <a name="step-1"></a>1. 将模型导出成SavedModel
[官方文档][official-doc]中的[示例代码][mnist_saved_model]给出了一个完整的模型创建和导出过程，但是如果你已经有了一个模型，你需要的只是以下几行代码（这段代码摘取自我的[人脸识别项目][my-face-recognize]中的convert_pretrained_model_to_savedModel.py，他讲一个预先训练好的模型转换成TF serving可用的模型）：

```python
  # export model to savedmodel

  # This is an path string like './model/1'
  export_model_path = os.path.join(EXPORT_PATH, str(MODEL_VERSION))  
  print('Model saved to:', export_model_path)
  builder = tf.saved_model.builder.SavedModelBuilder(export_model_path)

  # SavedModel need tensor info like dtype and shape, this method build tensor_info protobuf for us.
  # For we need build tensor_info protobuf for all the input and output tensor.
  tensor_info_x = tf.saved_model.utils.build_tensor_info(images_placeholder)
  tensor_info_train = tf.saved_model.utils.build_tensor_info(phase_train_placeholder)
  tensor_info_y = tf.saved_model.utils.build_tensor_info(embeddings)

  # a signature here is like a method signature used for gRPC call.
  # we need specify the input, out, put and method name.
  prediction_signature = (
      tf.saved_model.signature_def_utils.build_signature_def(
          inputs={'images': tensor_info_x, 'is_training': tensor_info_train},
          outputs={'scores': tensor_info_y},
          method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))
  

  # build the model with previous signatrue, I only have one signature, so I make it the default one.               
  # the legacy_init_op is used for compatibility with model generated by tf<1.2
  legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
  builder.add_meta_graph_and_variables(
      sess, [tf.saved_model.tag_constants.SERVING],
      signature_def_map={
          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
              prediction_signature,
      },
      legacy_init_op=legacy_init_op)

  builder.save()
```

当模型导出成功之后，你的模型目录应该如下图所示：

![导出模型目录](/images/saved_model.png?style=center "导出模型目录结构")


### <a name="step-2"></a>2. 安装，启动TF serving

这一部分比较复杂，我记得我去年尝试使用TF serving时就卡在了这一步，因为当时我的电脑没有足够的内存来编译它。然后去年Google中国开发者大会时我跟他们提了没有预编译的TF serving这个事，他们的主管很震惊还表示一定会把事情提上优先级。不过这些都是过去，现在是2018了，这些问题也都已经得到解决。

如果你的电脑配置不错，可以直接按照官方文档，下载Bazel和各种依赖，然后自己编译TF serving。但是如果你和我一样使用的是笔记本，那么docker+安装预编译包会是一个很好的选择。这一部分教程需要你有一点点Docker知识，教程基于我在Mac OS 上的操作，其他系统可能会有些许不同。

  1. 首先下载[基础Docker文件][basic-docker-file],理论上我们可以使用任何基于Ubuntu系统的docker，这里我使用Google提供的docker是为了确保不会有依赖缺失问题。
  2. 因为下载的知识一个docker file，我们需要在本地build docker image。在`Dockerfile.devel`所在文件路径下运行（不要丢掉命令最后的`.`）
      
      ```
      docker build --pull -t $USER/tensorflow-serving-devel -f Dockerfile.devel .
      ```

  3. 用交互模式运行编译好的docker镜像：

      ```
      docker run -it -p 9000:9000 $USER/tensorflow-serving-devel
      ```

  4. 在启动的docker中，使用apt-get安装预编译的TF serving：

      ```
      echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list
      
      curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -
      
      apt-get update && apt-get install tensorflow-model-server
      ```

  5. 在docker外将刚刚生成的SavedModel复制到docker中，在terminal中运行如下代码：

      ```
      docker cp ~/PROJECTS/python/Face-recognize-system/hchan/fake_facenet/tf_serving_model/ sleepy_hermann:/temp

      ```
  
  6. 最后一步，启动TF serving，并将生成的模型指定为提供服务的模型,这里的`model_name_str`可以自己指定，在后面客户端调用的时候会用得到。

```
    tensorflow_model_server --port=9000 --model_name=model_name_str --model_base_path=/temp/
```


[official-doc]: https://www.tensorflow.org/serving/serving_basic
[mnist_saved_model]: https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_saved_model.py
[my-face-recognize]: https://github.com/tw-bj-ml-community/Face-recognize-system
[basic-docker-file]: https://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile.devel