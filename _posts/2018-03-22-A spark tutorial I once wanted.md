---
layout: post
title:  "A spark tutorial I once wanted"
date:   2018-03-22 11:00:00 +0800
categories: Machine Learning
---

在我学习Spark和大数据之前，我一直想对大数据的技术栈有一个总体性的了解从而帮助我们做技术选型，而不是一下子钻进技术细节，API文档中去。我当时没有找到一篇完全符合我需求的文章，找到的文章要么太浅显要么太细节，这也是我写这篇文章的原因，希望本文能帮到和我有一样想法的人。

# Spark 解决了什么问题

随着计算机软硬件技术的提升，人类迎来了数据爆炸的时代，相对应的一些海量数据处理技术也被开发出来，其中Hadoop生态系统也已经成了海量数据处理中事实上的标准。不过Hadoop中的计算框架Map-Reduce也已经暴露出了一些问题：
1. Map-Reduce运算的延时比较高，不能适应越来越高的数据实时性要求。
2. API抽象层次低，应用开发难度高，不易上手。
3. 数据处理的中间结果保存在HDFS中
Spark在这方面进行了优化，提供了更加人性化的开发接口，将运算性能（尤其是迭代运算性能）大幅度提高，可以支持低延时数据分析应用。

# Spark 如何解决这些问题

目前的CPU和内存性能已经非常好，所以分布式系统的性能瓶颈主要来自于IO操作（文件读写，网络传输）和容错方面的考量。Map-Reduce计算框架通过移动运算减少了网络传输的需求，提升的分布式计算的性能。不过在容错方面，Map-Reduce通过将计算产生的中间数据存储到HDFS中来实现，这会一定程度上影响分布式运算的性能。Spark通过以下几个方面来提升运算性能：
1. 在Spark中，容错是通过引入函数式编程中的immutable变量实现的。在Spark中，所有的RDD都是不可变的，当worker出错时，只需根据原始数据重新运行数据转换操作即可恢复丢失掉的信息。
2. Spark把操作分为转化（Transformation）和行为（Action）两部分，当调用转换操作时不会有实际计算产生，只有在调用行为操作时才会执行运算，这给了Spark优化运算性能的机会。
3. 将数据流的处理方式与Scala的函数式编程思想结合起来，提供更友好易读的API，并提供多种开发语言的支持。

# 什么时候需要使用Spark

这是一个很难回答的问题，这里只能提供一些建议，
0. 你是不是真的需要大数据系统，如果现有技术能够很好地满足所有业务需求，不要使用包括Spark在内的大数据组件！
1. Map-Reduce计算框不能满足当前的也无需求时，可以考虑用Spark代替Map-Reduce计算框架。
1. 当你需要对大数据应用机器学习算法时，Spark能提供很好的性能和容易上手的API（MLLib）。
2. 当你需要高实时性的数据分析服务时，Spark（Spark Streaming）是一个不错的选择。不过Spark只能实现最低100ms左右的延迟，如果对实时性要求非常高，可以考虑Strom框架。
3. 如果你需要做交互时的数据分析，Spark是一个很好的选择，通过zeppelin可以方便的实现海量数据交互式分析。
4. 如果有基于图的分析需求，可以考虑Spark（GraphX）。





