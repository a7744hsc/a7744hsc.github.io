最近我在做一个人脸识别系统，就拿这个来举例吧。在做人脸识别的时候一般会把已知人员的面部特征存储起来，这里的特征时一个向量，存储的地方可以是数据库也可以直接存储在操作系统的文件系统中。 当得到一条人脸数据时（比如我们从摄像头中获得的人脸图像）我们会对图像进行特征抽取，然后再将特征和系统中已有的特征进行对比最终得到匹配结果。在这里我们假设每个人的人脸特征大小为1MB，实际上没有这么大。

背景介绍完了，我们先来说说大数据系统，Hadoop生态解决了什么问题。
最开始，我们的人脸识别系统只供家庭使用，那么系统中存储的特征可能不足十条，这时候特征存储在哪里都不会影响性能。后来我们吧系统推广到北京市，这时候就有几千万的数据要存储，我们需要存储的数据量就达到了几十TB，这对现在的服务器来说依然不成问题。再后来我们把这套系统推广到全球，这时我们的数据量就达到了PB级别，再往后如果我们能把系统推广到银河系，数据量更会大的不可想象。这时传统的文件系统和数据库难以直接处理这种问题，那么我们可能会选择把数据存储到多个数据库中，在数据对比时逐个对每个数据库中的数据进行对比。 显然这样做可维护性和扩展性都很差。这时HDFS出现了，他可以文件存储在计算机集群上